{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from linear_attention import LinearAttentionFunction\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        print(x.shape)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        print(qkv.shape)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Apply optimized linear attention\n",
    "        attn = LinearAttentionFunction.apply(q, k, v)\n",
    "        print(attn.shape)\n",
    "        \n",
    "        x = attn.transpose(1, 2).reshape(B, N, C)\n",
    "        print(x.shape)\n",
    "        # x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from linear_attention import LinearAttentionFunction\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'The dimension must be divisible by the number of heads'\n",
    "        \n",
    "        # Store dimensions for later use\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # QKV projection with proper initialization\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        torch.nn.init.normal_(self.qkv.weight, std=0.02)\n",
    "        if qkv_bias:\n",
    "            torch.nn.init.zeros_(self.qkv.bias)\n",
    "\n",
    "        # Output projection with proper initialization\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        torch.nn.init.normal_(self.proj.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input should be (batch_size, sequence_length, dimension)\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Verify input dimensions\n",
    "        assert C == self.dim, f'Input dimension {C} does not match expected {self.dim}'\n",
    "        \n",
    "        # Transform input into query, key, value tensors\n",
    "        # qkv shape: (batch_size, seq_len, 3, num_heads, head_dim)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Rearrange dimensions for attention computation\n",
    "        # Shape: (3, batch_size, num_heads, seq_len, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        \n",
    "        # Separate query, key, value\n",
    "        # Each has shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # Make tensors contiguous for efficient CUDA operations\n",
    "        q = q.contiguous()\n",
    "        k = k.contiguous()\n",
    "        v = v.contiguous()\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        print(f\"Query shape: {q.shape}\")\n",
    "        print(f\"Key shape: {k.shape}\")\n",
    "        print(f\"Value shape: {v.shape}\")\n",
    "        \n",
    "        try:\n",
    "            # Compute linear attention\n",
    "            # Expected output shape: (batch_size, num_heads, seq_len, head_dim)\n",
    "            attn = LinearAttentionFunction.apply(q, k, v)\n",
    "            \n",
    "            # Print shape for debugging\n",
    "            print(f\"Attention output shape: {attn.shape}\")\n",
    "            \n",
    "            # Verify output shape\n",
    "            expected_shape = (B, self.num_heads, N, self.head_dim)\n",
    "            assert attn.shape == expected_shape, \\\n",
    "                f\"Attention output shape {attn.shape} does not match expected {expected_shape}\"\n",
    "            \n",
    "            # Reshape to original dimensions\n",
    "            x = attn.transpose(1, 2).reshape(B, N, C)\n",
    "            \n",
    "            # Apply final projections\n",
    "            x = self.proj(x)\n",
    "            x = self.proj_drop(x)\n",
    "            return x\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\nError in attention computation:\")\n",
    "            print(f\"Input shapes:\")\n",
    "            print(f\"- Query: {q.shape}\")\n",
    "            print(f\"- Key: {k.shape}\")\n",
    "            print(f\"- Value: {v.shape}\")\n",
    "            if 'attn' in locals():\n",
    "                print(f\"Attention output shape: {attn.shape}\")\n",
    "            raise RuntimeError(\"Attention computation failed\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_shapes(batch_size=32, seq_len=128, dim=256, num_heads=8):\n",
    "    model = LinearAttention(dim=dim, num_heads=num_heads).cuda()\n",
    "    x = torch.randn(batch_size, seq_len, dim).cuda()\n",
    "    try:\n",
    "        output = model(x)\n",
    "        print(f\"Success! Output shape: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Run test with CUDA_LAUNCH_BLOCKING=1\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "test_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(\n",
    "    seq_lengths: List[int],\n",
    "    batch_size: int = 32,\n",
    "    dim: int = 256,\n",
    "    num_heads: int = 8,\n",
    "    num_warmup: int = 10,\n",
    "    num_repeats: int = 100,\n",
    "    device: str = \"cuda\"\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Benchmark linear attention against traditional attention across different sequence lengths.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of two DataFrames containing timing results and statistics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initialize models\n",
    "    linear_attn = LinearAttention(dim=dim, num_heads=num_heads).to(device)\n",
    "    trad_attn = TraditionalAttention(dim=dim, num_heads=num_heads).to(device)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        print(f\"Benchmarking sequence length: {seq_len}\")\n",
    "        \n",
    "        # Create input tensor\n",
    "        x = torch.randn(batch_size, seq_len, dim, device=device)\n",
    "        \n",
    "        # Warm up GPU\n",
    "        for _ in range(num_warmup):\n",
    "            _ = linear_attn(x)\n",
    "            _ = trad_attn(x)\n",
    "        \n",
    "        # Measure linear attention\n",
    "        torch.cuda.synchronize()\n",
    "        linear_times = []\n",
    "        for _ in range(num_repeats):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            \n",
    "            start.record()\n",
    "            _ = linear_attn(x)\n",
    "            end.record()\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            linear_times.append(start.elapsed_time(end))\n",
    "        \n",
    "        # Measure traditional attention\n",
    "        torch.cuda.synchronize()\n",
    "        trad_times = []\n",
    "        for _ in range(num_repeats):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            \n",
    "            start.record()\n",
    "            _ = trad_attn(x)\n",
    "            end.record()\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            trad_times.append(start.elapsed_time(end))\n",
    "            \n",
    "        # Calculate statistics\n",
    "        results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'linear_mean': np.mean(linear_times),\n",
    "            'linear_std': np.std(linear_times),\n",
    "            'trad_mean': np.mean(trad_times),\n",
    "            'trad_std': np.std(trad_times),\n",
    "            'speedup': np.mean(trad_times) / np.mean(linear_times)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrames for plotting\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    df_plot = pd.DataFrame([\n",
    "        {'seq_len': r['seq_len'], 'time': t, 'type': 'Linear Attention'}\n",
    "        for r in results\n",
    "        for t in [r['linear_mean']]\n",
    "    ] + [\n",
    "        {'seq_len': r['seq_len'], 'time': t, 'type': 'Traditional Attention'}\n",
    "        for r in results\n",
    "        for t in [r['trad_mean']]\n",
    "    ])\n",
    "    \n",
    "    return df_results, df_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_benchmark_results(df_plot: pd.DataFrame, df_results: pd.DataFrame):\n",
    "    \"\"\"Create visualization of benchmark results\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create subplot for timing comparison\n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.lineplot(data=df_plot, x='seq_len', y='time', hue='type', marker='o')\n",
    "    plt.title('Attention Computation Time vs Sequence Length')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Create subplot for speedup ratio\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(df_results['seq_len'], df_results['speedup'], marker='o')\n",
    "    plt.title('Speedup Ratio (Traditional / Linear)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Speedup Factor')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = [128, 256, 512, 1024, 2048, 4096]\n",
    "batch_size = 32\n",
    "dim = 256\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results, df_plot = benchmark_attention(\n",
    "    seq_lengths=seq_lengths,\n",
    "    batch_size=batch_size,\n",
    "    dim=dim,\n",
    "    num_heads=num_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBenchmark Results:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Create visualization\n",
    "plt = plot_benchmark_results(df_plot, df_results)\n",
    "plt.savefig('attention_benchmark.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nBenchmark visualization saved as 'attention_benchmark.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
